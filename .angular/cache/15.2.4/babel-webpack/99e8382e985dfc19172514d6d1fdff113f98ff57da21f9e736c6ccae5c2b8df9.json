{"ast":null,"code":"import * as i0 from \"@angular/core\";\nimport * as i1 from \"@angular/router\";\nimport * as i2 from \"../header/header.component\";\nimport * as i3 from \"../table-of-contents/table-of-contents.component\";\nimport * as i4 from \"../code-box/code-box.component\";\nexport let PysparkComponent = /*#__PURE__*/(() => {\n  class PysparkComponent {\n    constructor() {\n      this.tableOfContents = [['section1', 'Introduction'], ['section2', 'Download Spark'], ['section3', 'Add Spark to the Path'], ['section4', 'Pyspark Instllation'], ['section5', 'PySpark & MySql Connection']];\n      this.sections = [[]];\n    }\n    ngOnInit() {\n      this.loadScript('./assets/prism.js', 'js');\n      window.onload = () => {\n        this.loadScript('./assets/main.js', 'js');\n      };\n    }\n    loadScript(scriptUrl, Tipo) {\n      if (Tipo == 'js') {\n        const script = document.createElement('script');\n        script.type = 'text/javascript';\n        script.src = scriptUrl;\n        document.body.appendChild(script);\n      } else if (Tipo === 'css') {\n        const link = document.createElement('link');\n        link.rel = 'stylesheet';\n        link.type = 'text/css';\n        link.href = scriptUrl;\n        document.head.appendChild(link);\n      }\n    }\n    scrollToSection(elementId) {\n      const elementToScrollTo = document.getElementById(elementId);\n      if (elementToScrollTo) {\n        elementToScrollTo.scrollIntoView({\n          behavior: 'smooth'\n        });\n      }\n    }\n  }\n  PysparkComponent.ɵfac = function PysparkComponent_Factory(t) {\n    return new (t || PysparkComponent)();\n  };\n  PysparkComponent.ɵcmp = /*@__PURE__*/i0.ɵɵdefineComponent({\n    type: PysparkComponent,\n    selectors: [[\"app-pyspark\"]],\n    decls: 85,\n    vars: 1,\n    consts: [[\"lang\", \"en\"], [\"charset\", \"UTF-8\"], [\"name\", \"viewport\", \"content\", \"width=device-width, initial-scale=1.0\"], [1, \"global\"], [1, \"toc\"], [3, \"tableOfContents\", \"scrollToElement\"], [1, \"content\"], [\"id\", \"section1\"], [\"href\", \"https://es.wikipedia.org/wiki/Apache_Spark\"], [\"routerLink\", \"/practices/mysql\", \"routerLinkActive\", \"active\"], [\"id\", \"section2\"], [\"code\", \"wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\"], [\"code\", \"tar -xvzf spark-3.5.1-bin-hadoop3.tgz\"], [\"id\", \"section3\"], [\"code\", \"nano ~/.bashrc\"], [\"id\", \"section4\"], [\"code\", \"pip install pyspark\"], [\"href\", \"https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html\"], [\"code\", \"from pyspark.sql import SparkSession\\n                from pyspark.sql.functions import col\\n                \\n                # Create a SparkSession\\n                spark = SparkSession.builder.appName('Testing PySpark Example').getOrCreate()\"], [\"code\", \"sample_data = [{'name': 'John    D.'', 'age': 30},\\n                {'name': 'Alice   G.'', 'age': 25},\\n                {'name': 'Bob  T.'', 'age': 35},\\n                {'name': 'Eve   A.'', 'age': 28}]\\n              \\n              df = spark.createDataFrame(sample_data)\\n              df.show()\"], [\"src\", \"./assets/images/pyspark/df_output.png\", \"alt\", \"\", 2, \"display\", \"block\", \"margin-left\", \"auto\", \"margin-right\", \"auto\", \"max-width\", \"90%\"], [\"id\", \"section5\"], [\"href\", \"https://dev.mysql.com/downloads/connector/j/\"], [\"code\", \"dpkg -i mysql-connector-j_8.2.0-1debian11_all.deb\"], [\"code\", \"def mysql_spark_connection(spark_instance, user:str,password:str,host:str,port:str,database:str,table_name:str):\\n                        # Define JDBC connection properties\\n                        jdbc_url = f'jdbc:mysql://{host}:{port}/{database}'\\n                        connection_properties = {\\n                            'user': user,\\n                            'password': password,\\n                            'driver': 'com.mysql.cj.jdbc.Driver'\\n                        }\\n                        content = spark_instance.read.jdbc(url=jdbc_url,table=table_name, properties=connection_properties)\\n                        return content\"], [\"code\", \"movies_df=mysql_spark_connection(spark,'sqluser','mysqlpass','localhost','33060','MY_DATABASE','MOVIES')\"]],\n    template: function PysparkComponent_Template(rf, ctx) {\n      if (rf & 1) {\n        i0.ɵɵelementStart(0, \"html\", 0)(1, \"head\");\n        i0.ɵɵelement(2, \"meta\", 1)(3, \"meta\", 2);\n        i0.ɵɵelementStart(4, \"title\");\n        i0.ɵɵtext(5, \"Document\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(6, \"body\");\n        i0.ɵɵelement(7, \"app-header\");\n        i0.ɵɵelementStart(8, \"div\", 3)(9, \"div\", 4)(10, \"app-table-of-contents\", 5);\n        i0.ɵɵlistener(\"scrollToElement\", function PysparkComponent_Template_app_table_of_contents_scrollToElement_10_listener($event) {\n          return ctx.scrollToSection($event);\n        });\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(11, \"div\", 6)(12, \"div\", 7)(13, \"h2\");\n        i0.ɵɵtext(14, \" Introduction \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(15, \"p\");\n        i0.ɵɵtext(16, \" Apache Spark is an open-source cluster computing framework. It was originally developed at the University of California, at the AMPLab in Berkeley. The code base of the Spark project was later donated to the Apache Software Foundation, which has maintained it since then. \");\n        i0.ɵɵelementStart(17, \"a\", 8);\n        i0.ɵɵtext(18, \"Wikipedia\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(19, \"p\");\n        i0.ɵɵtext(20, \" In this practice i going to show you to install PySpark and to make an integration with MySql using the cloud shell. So, i have a little tutorial for cloud shell, if you need, in the introduction section of this practice: \");\n        i0.ɵɵelementStart(21, \"a\", 9);\n        i0.ɵɵtext(22, \"Setup MySql environment\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(23, \". \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(24, \"p\");\n        i0.ɵɵtext(25, \" Spark needs java to works, but in cloud shell you don't have to worry about that. I just download Spark from its web page, add the environment variable to the path and install PySpark. \");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(26, \"div\", 10)(27, \"h2\");\n        i0.ɵɵtext(28, \"Download Spark\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(29, \"p\");\n        i0.ɵɵtext(30, \" In the path '/home/<username>/' create a folder called Spark, open the terminal in this folder and then run the next commands \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(31, \"app-code-box\", 11)(32, \"app-code-box\", 12);\n        i0.ɵɵelementStart(33, \"p\");\n        i0.ɵɵtext(34, \"The spark version i am using is clearly '3.5.1'. The java and python version in cloud shell are '17.0.10 and '3.9.2', respectively.\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(35, \"div\", 13)(36, \"h2\");\n        i0.ɵɵtext(37, \"Add Spark to the Path\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(38, \"p\");\n        i0.ɵɵtext(39, \" Now, open the '.bashrc' and sroll to the last line. You can use \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(40, \"app-code-box\", 14);\n        i0.ɵɵelementStart(41, \"p\");\n        i0.ɵɵtext(42, \" Or just open the file with Code OSS. Now, define the 'SPARK_HOME' variable, just paste \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(43, \"ul\")(44, \"li\");\n        i0.ɵɵtext(45, \" \\\"SPARK_HOME=/home/<username>/Spark/spark-3.5.1-bin-hadoop3\\\" \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(46, \"li\");\n        i0.ɵɵtext(47, \" \\\"export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\\\" \");\n        i0.ɵɵelementEnd()()();\n        i0.ɵɵelementStart(48, \"div\", 15)(49, \"h2\");\n        i0.ɵɵtext(50, \"PySpark Installation\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(51, \"p\");\n        i0.ɵɵtext(52, \" Following with the installation. Create a python environment and then install the PySpark framework \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(53, \"app-code-box\", 16);\n        i0.ɵɵelementStart(54, \"p\");\n        i0.ɵɵtext(55, \" Next, open a Jupyter notebook in Code OSS, just create a '.ipynb' file and let the editor install the requirements that the editor needs. Don't forget select the environment where you install PySpark in Code OSS, you can add your environment using ctrl + shift + p. \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(56, \"p\");\n        i0.ɵɵtext(57, \" After that, you can run the next piece of code in spark, i recommend see the \");\n        i0.ɵɵelementStart(58, \"a\", 17);\n        i0.ɵɵtext(59, \"documentation\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(60, \". \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(61, \"app-code-box\", 18)(62, \"app-code-box\", 19);\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(63, \"p\");\n        i0.ɵɵtext(64, \" The output must look like this: \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(65, \"img\", 20);\n        i0.ɵɵelementStart(66, \"div\", 21)(67, \"h2\");\n        i0.ɵɵtext(68, \"PySpark & MySql Connection\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(69, \"p\");\n        i0.ɵɵtext(70, \" In this last part. You need a minimun background in sql. Just create a database, then create a table and insert a pair of rows for the test. \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(71, \"p\");\n        i0.ɵɵtext(72, \" The mysql connector can be download from \");\n        i0.ɵɵelementStart(73, \"a\", 22);\n        i0.ɵɵtext(74, \"here\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(75, \". Then, put the .deb in any folder open the terminal there and run \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(76, \"app-code-box\", 23);\n        i0.ɵɵelementStart(77, \"p\");\n        i0.ɵɵtext(78, \" My mysql version is 8.2.0 and i select the .deb file for debian 11 because it wokrs. But more important, check if 'mysql-connector-java-8.2.0.jar' exists in '/usr/share/java' after run the command above. Because the next part is in your Jupyter notebook. Copy the next function \");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(79, \"app-code-box\", 24);\n        i0.ɵɵelementStart(80, \"p\");\n        i0.ɵɵtext(81, \"Finally, use the function passing the user and password, in my case\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(82, \"app-code-box\", 25);\n        i0.ɵɵelementStart(83, \"P\");\n        i0.ɵɵtext(84, \"In the example the table name parameter 'MOVIES' can be changed by a query like '(query sentence) as table'. That's all, if you run 'movies_df.show()' spark display your table.\");\n        i0.ɵɵelementEnd()()()()()();\n      }\n      if (rf & 2) {\n        i0.ɵɵadvance(10);\n        i0.ɵɵproperty(\"tableOfContents\", ctx.tableOfContents);\n      }\n    },\n    dependencies: [i1.RouterLink, i1.RouterLinkActive, i2.HeaderComponent, i3.TableOfContentsComponent, i4.CodeBoxComponent],\n    styles: [\".global[_ngcontent-%COMP%]{display:flex;flex-direction:row;width:100%;height:90vh;background-color:#000;position:relative}.toc[_ngcontent-%COMP%]{display:flex;flex:1.1}.content[_ngcontent-%COMP%]{padding:2.1%;flex:3.5;border-style:groove;border-color:gray;border-top-right-radius:20px;border-bottom-right-radius:20px;background-color:#fff;overflow-y:scroll}p[_ngcontent-%COMP%]{text-indent:2em}\"]\n  });\n  return PysparkComponent;\n})();","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}